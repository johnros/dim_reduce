\pdfoutput=1
\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{url}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lineno}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{marginnote}
\AtBeginDocument{\let\textlabel\label}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black}



\author{Jonathan Rosenblatt \\ Ben Gurion University}

%% OPTIONAL MACRO DEFINITIONS
\input{dim_reduce_commands}

\title{Dimensionality Reduction}


\begin{document}

\maketitle

\begin{example}[BMI]
	Consider the heights and weights of a sample of individuals. 
	The data may seemingly reside in $2$ dimensions but given the height, we have a pretty good guess of a person's weight, and vice versa. 
	We can thus state that heights and weights are not really two dimensional, but roughly lay on a $1$ dimensional subspace of $\reals^2$. 
\end{example}


\begin{example}[IQ]
	Consider the correctness of the answers to a questionnaire with $p$ questions. 
	The data may seemingly reside in a $p$ dimensional space, but assuming there is such a thing as ``skill'', then given the correctness of a person's reply to a subset of questions, we have a good idea how he scores on the rest. 
	Put differently, we don't really need a $200$ question questionnaire-- $100$ is more than enough.
	If skill is indeed a one dimensional quality, then the questionnaire data should organize around a single line in the $p$ dimensional cube. 
\end{example}


\begin{example}[Blind signal separation]
	Consider $n$ microphones recording an individual. 
	The digitized recording consists of $p$ samples. 
	Are the recordings really a shapeless cloud of $n$ points in $\reals^p$?
	Since they all record the same sound, one would expect them to arrange around a single 
\end{example}
		
	


\section{Terminology}

\begin{tcolorbox}
\begin{description}
	\item[Manifold] A space which is regular enough so that it is \emph{locally} has all the properties of a linear space. 
	
	\item[Embedding] A mapping from one space into another. 
	
	\item[Linear Embedding] An embedding done via a linear operation (thus representable by a matrix). 
	
	\item[Generative Model] Known to statisticians as the \emph{sampling distribution}. 
	The assumed stochastic process that generated the observed data. 
	
\end{description}
\end{tcolorbox}





\section{Principal Component Analysis}
\label{sec:pca}

\emph{Principal Component Analysis} (PCA) is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of \emph{
	discrete Karhunen–Loève Transform; 
	Hotteling Transform; 
	Proper Orthogonal Decomposition (POD); 
	Eckart–Young Theorem; 
	Schmidt–Mirsky Theorem;  
	Empirical Orthogonal Functions; 
	Empirical Eigenfunction Decomposition;  
	Empirical Component Analysis;  
	Quasi-Harmonic Modes;  
	Spectral Decomposition;  
	Empirical Modal Analysis}, 
and possibly more\footnote{\url{http://en.wikipedia.org/wiki/Principal_component_analysis} }.
The many names are quite interesting as they offer an insight into the different problems that led to its (re)discovery.

Starting with an example, consider human height and weight data. 
While clearly two dimensional data, you don't really need both to understand how ``big'' are the people in the data. 
This is because, height and weight vary mostly along a single dimension, which can be interpreted as the ``bigness'' of an individual. 
This is why, physicians use the Body Mass Index (BMI) as an indicator of size, instead of a two-dimensional measurement.
Assume you now wish to give each individual a size score, that is a linear combination of height and weight: PCA does just that. It returns the linear combination that has the most variability, i.e., the combination which best distinguishes between individuals. 

The variance maximizing motivation above was the one that guided Hotelling \citep{hotelling_analysis_1933}.
About $30$ years before him, Karl Pearson \citep{pearson_liii._1901} derived the same procedure with a different motivation in mind. Pearson was also trying to give each individual a score. He did not care about variance maximization, however. He simply wanted a small set of coordinates in some (linear) space that approximates the original features well. As it turns out, the best linear-space approximation of $X$ is also the variance maximizing one. Pearson and Hotelling thus arrived to the same solution, with different motivations. 





\subsubsection{Scree Plot}
\label{sec:scree_plot}
[TODO]



\subsubsection{Bi Plot}
\label{sec:bi_plot}
[TODO]



\subsubsection{Intuition}
\label{sec:pca_intuition}

Notice we have currently offered two motivations for PCA: 
(i) Find linear combinations that best distinguish between observations, i.e., maximize variance. 
(ii) Find the linear subspace the bets approximates the data.
The reason these two problems are equivalent, is due to the use of the squares error.
Informally speaking, the data has some total variance. This variance can be decomposed into the part captured in $\manifold$, and the part not captured\footnote{Analogous to $SST=SSR+SSE$ in linear regression.}. 
Since the variance in the data consists of sums of squares, minimizing the distance from $X$ to $\manifold$, is the same as maximizing the variance of $X \project \manifold$, since their sum is fixed.



\subsubsection{Mathematics of PCA}
\label{sec:pca_mathematics}
We now present the derivation of PCA from the two different motivations.




\paragraph{The Variance Maximization View}
Starting with the first principal component.
If $\cov{x}=\Sigma$ then for a fixed vector $v$: $\cov{vx}=v \Sigma v'$.
Maximizing w.r.t. $v$ does not make sense as $\cov{vx}$ will explode. 
It is most convenient, mathematically, to constrain the $l_2$ norm: $\normII{v}^2=1$.
Maximizing under a constraint, using Lagrange-Multipliers: 
\begin{align}
\argmax{v}{v \Sigma v' - \lambda (\normII{v}^2-1)}.
\end{align}
Differentiating w.r.t $v$ and equating zero: 
\begin{align}
(\Sigma- \lambda I)v'=0
\end{align}
So the $P$ solutions for $v$ are the eigen-vectors of $\Sigma$. Which of them to pick? 
To find a \emph{global} maximum we return to the original problem, as plug our result:
\begin{align}
\label{eq:pca_maximal_variance}
\argmax{v:\normII{v}^2=1}{v \Sigma v' }=\argmax{\lambda}{v \lambda v' }
\end{align}
so that the global maximum is obtained with the largest eigen-value $\lambda$.

Readers familiar with matrix norms will recognize that this is simply the derivation of the operator norm of $\Sigma$.

The second principal component can be found by solving the same problem, with the additional constraint of $v_2$ orthogonal to $v_1$.

The last missing ingredient is that instead of the true covariance between the features, $\Sigma$, we use the (scaled) empirical covariance $X'X$.




\paragraph{The Linear-Space Embedding View}
We now seek to find a sequence of $p$ approximations to $X$ that lay in $1,\dots,p$ dimensional linear subspaces, with respect to a least squares loss. For simplicity of exposition, we will assume that $X$ has been mean centred. 
The $\rank$'th problem to solve is thus
\begin{align}
\label{eq:pca_erm}
\argmin{\hyp_\rank}{\normF{X-\hyp_\rank(X)}}.
\end{align}
Since $\hyp_rank$ is a map from $\reals^p$ to some rank-$\rank$ linear subspace, it must have the form $\hyp_\rank(X)=\projectMat_\rank X$ where $\projectMat_\rank$ is a $n \times n$ matrix of rank $\rank$.
Since Eq.(\ref{eq:pca_erm}) minimizes sums of (squared) Euclidean distances, $\projectMat_\rank$ has to be an orthogonal projection, thus symmetric. As such it can decomposed into an outer product $\projectMat_\rank=V_\rank V'_\rank$ where $V_\rank$ is full rank $n \times \rank$ matrix \citep[Eq.(5.13.4)]{meyer_matrix_2001}.
Under the $\rank$-space constraint, and squared error, Eq.(\ref{eq:pca_erm}) collapses to 
\begin{align}
\label{eq:pca_erm2}
\argmin{V_\rank}{\normF{X-V_\rank V'_\rank(X)}}.
\end{align}
Using some algebraic identities \cite[Eq.(23.3)]{shalev-shwartz_understanding_2014} Eq.(\ref{eq:pca_erm2}) is equivalent to 
\begin{align}
\label{eq:pca_erm3}
\argmax{V_\rank}{\Tr(V'_\rank XX' V_\rank)}.		
\end{align}
At this point we should note that the linear-space embedding problem has collapsed to the variance maximization problem! 
If you do not see this, just set $\rank=1$ and compare to Eq.(\ref{eq:pca_maximal_variance}), recalling that $X'X$ estimates the features' covariance $\Sigma$.






\subsubsection{PCA as a Graph Method}
\label{remark:pca_as_graph}
Starting from the maximal variance motivation, it is perhaps not surprising that PCA depends only on the similarities between features, as measured by their empirical covariance. The linearity of the target manifold was there by assumption. 

Following the linear-space embedding motivation, it is was surprising that the solutions depend only on the empirical covariances. This fact can be attributed to the use of squared error loss, which implied we were trying to decompose the total variance into the part in $\manifold_\rank$ and the orthogonal part.

From both motivations we see that the values of $X$ are of no importance given $X'X$, which can be informally thought of as a sufficient statistic\footnote{It is not a proper sufficient statistic as no generative model has been assumed.}.  

In-turn, $X'X$ depends only on the empirical covariances between \emph{individuals} ($\similaritys=XX'$), or on the Euclidean distances between individuals ($\dissimilaritys=(\norm{x_i-x_j})$).

The building blocks of all these graph-based dimensionality reduction methods are:
\begin{enumerate}
	\item Compute some similarity graph $\similaritys$ (or dissimilarity graph $\dissimilaritys$) from the raw features.
	\item Call upon graph embedding theory to map the data points into the target manifold $\manifold$.
\end{enumerate}
The fact that the linear-space embedding of the data depends only some similarity graph has laid a bridge between feature embedding, such as PCA, and \emph{graph embedding} methods such as MDS (\S\ref{sec:mds}).
Moreover, it has opened the door for replacing the covariance similarity, with many other similarity measures. 
Classic MDS (\S\ref{sec:mds}) is simply PCA when starting from $\similaritys$, thus viewed as a graph embedding problem.
kPCA (\S\ref{sec:kpca}) plugs kernel similarities (\S\ref{apx:rkhs}) instead of covariance similarities. 
Isomap (\S\ref{sec:isomap}), LocalMDS (\S\ref{sec:localMDS}), and LLE (\S\ref{sec:lle}) follow a similar motivation using \emph{local} measures of similarity.
Spectral Clustering (\S\ref{sec:spectral_clustering}) does some linear-space embedding \`a-la PCA, then wrapping up with a clustering algorithm in $\manifold$ \`a-la K-means. 



We now prove that the PCA solution can be cast in terms of the covariance between individuals ($\similaritys=XX'$) or the Euclidean distances ($\dissimilaritys=\norm{x_i-x_j}$).
In particular, we show that all the information on the location (mean) of $X$, needed for the PCA reconstruction, is actually encoded in $\similaritys$ (or $\dissimilaritys$).

The following exposition takes from \cite[Section 18.5.2]{hastie_elements_2003}


\paragraph{PCA with the Covariance Similarity Graph}
To begin, we need to cast the solution to the PCA problem in Eq.(\ref{eq:pca_erm3}) using the Singular Value Decomposition (SVD).\marginnote{SVD}

\begin{definition}[SVD]
	Any $n \times p$ matrix $X$, can be decomposed into $X=UDV'$ where 
	$U$ is an $n \times p$ orthogonal matrix ($U'U=I_p$); 
	$D$ is a $p \times p$ diagonal matrix with diagonal elements $d_1 \geq d_2 \geq \dots \geq d_p$;
	$V$ is a $p \times p$ orthogonal matrix ($V'V=I_p$).
\end{definition}

For mean centered $X$, the series of embeddings $\hyp_\rank(X)$ for $\rank=1,\dots,\pagebreak$ resulting from Eq.(\ref{eq:pca_erm3}) is given by $\hyp_\rank(X)=U_\rank D_\rank$, where $U_\rank$ $D_\rank$ are the $\rank$ leading columns of $U$ and $D$ respectively. $UD$ is thus the sequence of all solutions.

Now denoting $\similaritys=XX'$ and calling SVD: $\similaritys=U D^2 U'$. 
We thus see that by decomposing $\similaritys$ we can recover $U$, $D$, and thus $\hyp_\rank(X)$.

If $X$ is not mean centred, the relation still holds, but we skip the presentation.


\paragraph{PCA with the Euclidean Distance Dissimilarity Graph}
Can we convert Euclidean distances to empirical covariances? Yes!

Denote the matrix of distances of a non-centred $X$: $\dissimilaritys^2=(\norm{x_i-x_j}^2)$.
\begin{align}
\dissimilaritys^2_{i,j} =& \norm{x_i-x_j}^2 \\
=& \normII{x_i-\bar{x}}+\normII{x_j-\bar{x}}-2 \scalar{x_i-\bar{x}}{x_j-\bar{x}} \\
=& \normII{x_i-\bar{x}}+\normII{x_j-\bar{x}}-2 \similaritys_{i,j}
\end{align}
where $\similaritys_{i,j}$ is the empirical covariance between individual $i$ and $j$.
We thus have 
\begin{align}
\similaritys= - (I-M) \frac{\dissimilaritys^2}{2} (I-M)
\end{align}
where $M$ is the centring matrix: $M:= \frac{1}{n} \ones \ones'$, and $\ones$ an $n$ vector of $1$'s.







\section{FA}




\section{ICA}






\newpage
\bibliographystyle{abbrvnat}
\bibliography{dim_reduce.bib}



\end{document}
